# ğŸ¦ğŸ“Š OpenClawmetry

**OpenClaw + OpenTelemetry = Full Observability for AI Agents**

<p align="center">
  <img src="https://raw.githubusercontent.com/openclaw/openclaw/main/docs/assets/openclaw-logo-text.png" alt="OpenClawmetry" width="400">
</p>

<p align="center">
  <strong>See Everything Your AI Agent Does</strong>
</p>

<p align="center">
  <a href="LICENSE"><img src="https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge" alt="MIT License"></a>
  <img src="https://img.shields.io/badge/OpenTelemetry-enabled-blueviolet?style=for-the-badge&logo=opentelemetry" alt="OpenTelemetry">
  <img src="https://img.shields.io/badge/node-%3E%3D22-brightgreen?style=for-the-badge&logo=node.js" alt="Node.js">
</p>

---

OpenClawmetry is a fork of [OpenClaw](https://github.com/openclaw/openclaw) with **comprehensive distributed tracing** built in. See exactly what your AI agent is doingâ€”every LLM call, tool execution, and message flowâ€”visualized in Jaeger or your preferred observability platform.

## âœ¨ Why OpenClawmetry?

When AI agents make decisions, execute tools, and call LLMs, you need visibility into:

- â±ï¸ **Where is time being spent?** â€” LLM calls vs tool execution vs routing
- ğŸ” **What tools are being used?** â€” And are they succeeding or failing?
- ğŸ§  **How many tokens are consumed?** â€” Track costs across requests
- ğŸ”€ **How are messages routed?** â€” Which agent handles what?
- ğŸ› **Where do errors occur?** â€” Pinpoint failures in complex workflows

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OpenClawmetry Gateway                         â”‚
â”‚  ws://127.0.0.1:18789                                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Channel  â”‚â†’ â”‚ Router   â”‚â†’ â”‚ Agent    â”‚â†’ â”‚ Skills/  â”‚       â”‚
â”‚  â”‚ Handlers â”‚  â”‚          â”‚  â”‚ Brain    â”‚  â”‚ Tools    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â†“              â†“             â†“             â†“              â”‚
â”‚  [span:         [span:       [span:        [span:              â”‚
â”‚   message.flow]  message.route] llm.call]   tool.*]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Jaeger      â”‚
                    â”‚  localhost:16686â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/percepteye-ai/openclawmetry.git
cd openclawmetry
pnpm install
pnpm build
```

### 2. Start Jaeger

```bash
docker compose -f docker-compose.otel.yml up -d
```

### 3. Run with Tracing

```bash
# Development mode
npm run gateway:dev:otel

# Production mode
npm run gateway:otel
```

### 4. View Traces

Open **http://localhost:16686** â†’ Select `openclaw-gateway` â†’ Click **Find Traces**

---

## ğŸ“Š What Gets Traced

| Span | Description | Key Attributes |
|------|-------------|----------------|
| `message.flow` | Complete message lifecycle | `session_id`, `channel`, `message_length` |
| `message.route` | Agent routing decision | `agent_id`, `matched_by`, `session_key` |
| `tool.*` | Tool/skill execution | `tool_name`, `duration_ms`, `success` |
| `llm.completion` | LLM API calls | `model`, `input_tokens`, `output_tokens` |
| `HTTP *` | Outbound HTTP | `url`, `status_code` |
| `fetch` | Fetch API calls | `url`, `method` |

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OTEL_EXPORTER_OTLP_ENDPOINT` | `http://localhost:4318/v1/traces` | OTLP collector endpoint |
| `OPENCLAW_VERSION` | `1.0.0` | Service version in traces |
| `NODE_ENV` | `development` | Deployment environment |

### Production Backends

<details>
<summary><strong>Datadog</strong></summary>

```bash
export OTEL_EXPORTER_OTLP_ENDPOINT="https://http-intake.logs.datadoghq.com/api/v2/otlp"
export OTEL_EXPORTER_OTLP_HEADERS="DD-API-KEY=your_api_key"
```
</details>

<details>
<summary><strong>New Relic</strong></summary>

```bash
export OTEL_EXPORTER_OTLP_ENDPOINT="https://otlp.nr-data.net:4318"
export OTEL_EXPORTER_OTLP_HEADERS="api-key=YOUR_LICENSE_KEY"
```
</details>

<details>
<summary><strong>Grafana Cloud</strong></summary>

```bash
export OTEL_EXPORTER_OTLP_ENDPOINT="https://otlp-gateway-prod-us-central-0.grafana.net/otlp"
export OTEL_EXPORTER_OTLP_HEADERS="Authorization=Basic base64(instanceId:apiKey)"
```
</details>

---

## ğŸ“ Instrumentation Files

| File | Purpose |
|------|---------|
| `otel-setup.js` | SDK initialization & auto-instrumentation |
| `otel-openclaw.js` | Custom span helpers for LLM/tools |
| `otel-workflow.js` | Full workflow tracing with session correlation |
| `src/otel-instrumentation.ts` | TypeScript instrumentation integrated into source |
| `docker-compose.otel.yml` | Jaeger all-in-one for local development |

---

## ğŸ”§ Extending Instrumentation

Add custom spans anywhere in the codebase:

```typescript
import {
  tracer,
  traceToolExecution,
  startMessageFlow,
  endMessageFlow
} from './otel-instrumentation.js';

// Wrap a function with tool tracing
const tracedFn = traceToolExecution('my-tool', myToolFunction);

// Or create manual spans
const span = tracer.startSpan('custom.operation');
try {
  // ... your code
  span.setStatus({ code: SpanStatusCode.OK });
} finally {
  span.end();
}
```

---

## ğŸ§ª Verify Installation

```bash
# Quick test
node test-otel.js

# Check Jaeger at http://localhost:16686
```

---

## ğŸ¦ About OpenClaw

OpenClawmetry is built on [OpenClaw](https://github.com/openclaw/openclaw), a personal AI assistant you run on your own devices. It supports WhatsApp, Telegram, Slack, Discord, Signal, iMessage, and more.

- [OpenClaw Docs](https://docs.openclaw.ai)
- [Getting Started](https://docs.openclaw.ai/start/getting-started)

---

## ğŸ“œ License

MIT License â€” see [LICENSE](LICENSE) for details.

- **OpenClaw** Â© 2025 Peter Steinberger
- **OpenTelemetry Instrumentation** Â© 2026 Srini

---

## ğŸ™ Acknowledgments

- [OpenClaw](https://github.com/openclaw/openclaw) â€” The AI agent framework
- [OpenTelemetry](https://opentelemetry.io/) â€” The observability standard
- [Jaeger](https://www.jaegertracing.io/) â€” Distributed tracing UI

---

<p align="center">
  <strong>Built with ğŸ¦ and ğŸ“Š by <a href="https://github.com/percepteye-ai">PerceptEye</a></strong>
</p>
